{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import imageio\n",
    "import dlib\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import requests\n",
    "from typing import List\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Conv3D, LSTM, Dense, Dropout, Bidirectional, MaxPool3D, Activation, Reshape, SpatialDropout3D, BatchNormalization, TimeDistributed, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.models import load_model\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = pathlib.Path('data')  # Use pathlib for cross-platform compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only ran once to download the data from the google drive and github. The data is already in the repository.\n",
    "```python\n",
    "url = 'https://drive.google.com/uc?id=1YlvpDLix3S-U8fd-gqRwPcWXAXm8JwjL'\n",
    "output = 'data.zip'\n",
    "gdown.download(url, output, quiet=False)\n",
    "gdown.extractall('data.zip')\n",
    "\n",
    "url = \"https://github.com/JeffTrain/selfie/raw/master/shape_predictor_68_face_landmarks.dat\"\n",
    "file_name = \"shape_predictor_68_face_landmarks.dat\"\n",
    "response = requests.get(url)\n",
    "with open(file_name, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "I am using the following excerpt from the paper as a reference to preprocess the data:\n",
    "\n",
    "All videos are 3 seconds long with a frame rate of 25fps. The videos were processed with the DLib face detector, and the iBug face landmark predictor (Sagonas et al., 2013) with 68 landmarks coupled with an online Kalman Filter. Using these landmarks, we apply an affine transformation to extract a mouth-centred crop of size 100 Ã— 50 pixels per frame. We standardise the RGB channels over the whole training set to have zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_face_detector = dlib.get_frontal_face_detector()\n",
    "dlib_facelandmark = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "def preprocess_video_frame(frame):\n",
    "    # Convert frame to grayscale\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = hog_face_detector(frame_gray)\n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "    face = faces[0]\n",
    "    landmarks = dlib_facelandmark(frame_gray, face)\n",
    "    if landmarks:\n",
    "        mouth_points = np.array([[p.x, p.y] for p in landmarks.parts()[48:68]])\n",
    "        x, y, w, h = cv2.boundingRect(mouth_points)\n",
    "        if w > 0 and h > 0:  \n",
    "            # Check if bounding box is valid\n",
    "            mouth = frame_gray[y:y+h, x:x+w]\n",
    "            mouth_resized = cv2.resize(mouth, (100, 50))\n",
    "            return mouth_resized\n",
    "    return None\n",
    "\n",
    "def preprocess_video(path: str) -> List[np.ndarray]:\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            future = executor.submit(preprocess_video_frame, frame)\n",
    "            futures.append(future)\n",
    "        for future in futures:\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                frames.append(result)\n",
    "    cap.release()\n",
    "\n",
    "    if frames:\n",
    "        frames_np = np.stack(frames)\n",
    "        mean = np.mean(frames_np, axis=0)\n",
    "        std = np.std(frames_np, axis=0)\n",
    "        frames_standardized = (frames_np - mean) / std\n",
    "        return frames_standardized.tolist()\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='font-family:sans-serif'>  Creating a vocabulary\n",
    "This can give us values that can be passed to a loss function by tokenization. This method is inspired by this example https://keras.io/examples/audio/ctc_asr/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [letter for letter in \"abcdefghijklmnopqrstuvwxyz'!?1234567890 \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_num = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), \n",
    "    oov_token='')\n",
    "num_to_char = tf.keras.layers.StringLookup(\n",
    "    vocabulary=char_to_num.get_vocabulary(), \n",
    "    oov_token='', \n",
    "    invert=True)\n",
    "\n",
    "print(\n",
    "    f\"The vocabulary contains: {char_to_num.get_vocabulary()}, \\n\"\n",
    "    f\"And the size of the vocabulary is {char_to_num.vocabulary_size()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = 'data/s1/bbaf2n.mpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to find the name of the file by splitting as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.convert_to_tensor(test_path).numpy().decode('utf-8').split('/')[-1].split('.')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be applied on a larger scale to get the names of all the files in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the loading functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_alignments(path:str) -> List[str]: \n",
    "    with open(path, 'r') as f: \n",
    "        lines = f.readlines() \n",
    "    tokens = []\n",
    "    for line in lines:\n",
    "        line = line.split()\n",
    "        if line[2] != 'sil': \n",
    "            tokens = [*tokens,' ',line[2]]\n",
    "    return char_to_num(tf.reshape(tf.strings.unicode_split(tokens, input_encoding='UTF-8'), (-1)))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path: str): \n",
    "    path = bytes.decode(path.numpy())\n",
    "    file_name = path.split(os.sep)[-1].split('.')[0]\n",
    "    video_path = os.path.join(data_path, 's1', f'{file_name}.mpg')\n",
    "    alignment_path = os.path.join(data_path, 'alignments', 's1', f'{file_name}.align')\n",
    "    frames = preprocess_video(video_path) \n",
    "    alignments = load_alignments(alignment_path)\n",
    "    \n",
    "    return frames, alignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the results of the loading functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "frames, alignments = load_data(tf.convert_to_tensor(test_path))\n",
    "plt.imshow(frames[20])\n",
    "plt.show()\n",
    "print(tf.strings.reduce_join([num_to_char(word) for word in alignments]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mappable_function(path:str) ->List[str]:\n",
    "    result = tf.py_function(load_data, [path], (tf.float32, tf.int64))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the data pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.data.Dataset.list_files(str(data_path / 's1' / '*.mpg'))\n",
    "data = data.shuffle(500, reshuffle_each_iteration=False)\n",
    "data = data.map(mappable_function)\n",
    "# 75 frames, don't change the size of frames. 40 tokens in the alignments.\n",
    "data = data.padded_batch(2, padded_shapes=(([75, None, None], [40])))\n",
    "data = data.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data into training and validation\n",
    "train_data = data.take(450)\n",
    "val_data = data.skip(450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying the batching and the shape of the data works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames, alignments = data.as_numpy_iterator().next()\n",
    "val = data.as_numpy_iterator().next(); val[0]\n",
    "print(frames.shape, alignments.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.data.Dataset.list_files('./data/s1/*.mpg')\n",
    "data = data.shuffle(500, reshuffle_each_iteration=False)\n",
    "data = data.map(mappable_function)\n",
    "#75 frames, don't change size of frames. 40 tokens in the alignments.\n",
    "data = data.padded_batch(2, padded_shapes=(([75, None, None], [40])))\n",
    "data = data.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a gif out of a sample video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = str(data_path / 's1' / 'bbaf2n.mpg')\n",
    "frames = preprocess_video(sample)  # Ensure this returns a list of np.ndarray frames\n",
    "\n",
    "frames_array = np.array(frames)\n",
    "min_val = frames_array.min()\n",
    "max_val = frames_array.max()\n",
    "\n",
    "with imageio.get_writer(\"./mouth_movement.gif\", mode='I') as writer:\n",
    "    for frame in frames_array:\n",
    "        normalized_frame = ((frame - min_val) * (255 / (max_val - min_val))).astype('uint8')\n",
    "        writer.append_data(normalized_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the initial Neural Network:\n",
    "Using CTC loss to train the model. The model is a simple CNN with a GRU layer. We are using this, as I expect the later-recieved data to not be as clean as the data we are training on. In other words, the model should be constructed for non-alligned data although we are working with aligned training data.\n",
    "I will go back and set up a more complex model later, but for now, I will use this simple model to have something to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, output_size):\n",
    "    model = Sequential()\n",
    "    model.add(Conv3D(128, (3, 3, 3), activation='relu', padding='same', input_shape=input_shape))\n",
    "    model.add(MaxPool3D((1, 2, 2)))\n",
    "    model.add(Conv3D(256, (3, 3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPool3D((1, 2, 2)))\n",
    "    model.add(Conv3D(64, (3, 3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPool3D((1, 2, 2)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(Bidirectional(LSTM(256, return_sequences=True, kernel_initializer='Orthogonal')))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Bidirectional(LSTM(256, return_sequences=True, kernel_initializer='Orthogonal')))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(output_size, activation='softmax', kernel_initializer='he_normal'))\n",
    "    return model\n",
    "\n",
    "model = create_model(input_shape=(75, 50, 100, 1), output_size=char_to_num.vocabulary_size() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if the model will return an output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(val[0])\n",
    "tf.strings.reduce_join([num_to_char(word) for word in tf.argmax(yhat, axis=-1)[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traning the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 32:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the CTC loss function from https://keras.io/examples/audio/ctc_asr/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CTCLoss(y_true, y_pred):\n",
    "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "    loss = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProduceExample(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset.as_numpy_iterator()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        data = self.dataset.next()\n",
    "        yhat = model.predict(data[0])\n",
    "        decoded = tf.keras.backend.ctc_decode(yhat, input_length=np.ones(yhat.shape[0]) * yhat.shape[1], greedy=False)[0][0].numpy()\n",
    "        for i in range(len(yhat)):\n",
    "            print('Actual:', tf.strings.reduce_join([num_to_char(word) for word in data[1][i]]).numpy().decode('utf-8'))\n",
    "            print('Predicted:', tf.strings.reduce_join([num_to_char(word) for word in decoded[i]]).numpy().decode('utf-8'))\n",
    "            print('*'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_checkpoint = 'models/checkpoints.weights.h5'\n",
    "model.load_weights(latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=1e-3), loss=CTCLoss)\n",
    "checkpoint = ModelCheckpoint(os.path.join('models', 'checkpoints.weights.h5'), monitor='loss', save_weights_only=True, mode='min', verbose=1)\n",
    "schedule = LearningRateScheduler(scheduler, verbose=1)\n",
    "produce_example = ProduceExample(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_data, validation_data=val_data, epochs=128, callbacks=[checkpoint, schedule, produce_example])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('optimized_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
