Creating a model that is able to lipread from a video input.
Ideally this can be used to assist transcribing videos for the hearing impaired, or to help with video search by providing more data to compare against.

Ultimately I would like to create a model that can lipread in real time (from a videostream), but for now I will be focusing on creating a model that can lipread from a video file.

This project is inspired by the paper "LipNet: End-to-End Sentence-level Lipreading" by Yannis M. Assael, Brendan Shillingford, Shimon Whiteson, and Nando de Freitas. The paper can be found at https://arxiv.org/abs/1611.01599.